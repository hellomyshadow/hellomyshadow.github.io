<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="爬虫,">










<meta name="description" content="cookielibcookielib是自动处理cookie的模块，对应urllib2的处理器：HTTPCookieProcessor     1. cookielib：主要作用是提供用于存储cookie的对象；     2. HTTPCookieProcessor：处理cookie对象，并构建Handler对象； 1. cookielib的主要对象：CookieJar、FileCookieJar、">
<meta name="keywords" content="爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫基础-2">
<meta property="og:url" content="http://hellomyshadow.github.io/2017/10/05/爬虫基础-2/index.html">
<meta property="og:site_name" content="大麦田程序猿">
<meta property="og:description" content="cookielibcookielib是自动处理cookie的模块，对应urllib2的处理器：HTTPCookieProcessor     1. cookielib：主要作用是提供用于存储cookie的对象；     2. HTTPCookieProcessor：处理cookie对象，并构建Handler对象； 1. cookielib的主要对象：CookieJar、FileCookieJar、">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-03-16T08:47:33.004Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爬虫基础-2">
<meta name="twitter:description" content="cookielibcookielib是自动处理cookie的模块，对应urllib2的处理器：HTTPCookieProcessor     1. cookielib：主要作用是提供用于存储cookie的对象；     2. HTTPCookieProcessor：处理cookie对象，并构建Handler对象； 1. cookielib的主要对象：CookieJar、FileCookieJar、">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://hellomyshadow.github.io/2017/10/05/爬虫基础-2/">





  <title>爬虫基础-2 | 大麦田程序猿</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">大麦田程序猿</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hellomyshadow.github.io/2017/10/05/爬虫基础-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="大麦田怪圈">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大麦田程序猿">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">爬虫基础-2</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-05T00:00:00+08:00">
                2017-10-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="cookielib"><a href="#cookielib" class="headerlink" title="cookielib"></a>cookielib</h2><pre><code>cookielib是自动处理cookie的模块，对应urllib2的处理器：HTTPCookieProcessor
    1. cookielib：主要作用是提供用于存储cookie的对象；
    2. HTTPCookieProcessor：处理cookie对象，并构建Handler对象；
1. cookielib的主要对象：CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar
    1. CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向发送的HTTP请求添加
    cookie的对象；整个cookie都存储在内存中，CookieJar实例被垃圾回收后，cookie也将丢失；
    2. FileCookieJar(filename, delayload=None, policy=None)：CookieJar的子类，
    检索cookie信息并存储到文件中，delayload为True时支持延迟访问文件，只有在需要时才读取
    文件或者在文件中存储数据；
    3. MozillaCookieJar(filename,delayload=None,policy=None)：FileCookieJar的子类，
    用于兼容Mozilla浏览器的FileCookieJar；
    4. LWPCookieJar(filename, delayload=None, policy=None)：FileCookieJar的子类，
    用于兼容libwww-perl标准Set-Cookie3文件格式的FileCookieJar。
2. HTTPCookieProcessor
    1. 构建能保存cookie的处理器和Opener：
        cookie = cookielib.CookieJar()
        handler = urllib2.HTTPCookieProcessor(cookie)
        opener = urllib2.build_opener(handler)
    2. 发送登录请求，保存生成的cookie：
        url = &apos;http://www.renren.com/PLogin.do&apos; --&gt; 老版人人网没有动态token验证
        data = {&apos;email&apos;: &apos;人人网用户名&apos;, &apos;password&apos;: &apos;人人网密码&apos;}
        request = urllib2.Request(url, data=urllib.urlencode(data), headers)
        response = opener.open(request) --&gt; 发送POST请求，如果登录成功，则保存Cookie
    3. 使用cookie访问个人主页
        pro_url = &apos;http://www.renren.com/966033501/profile&apos; --&gt; 人人网个人主页
        pro_request = urllib2.Request(profile_url, headers)
        res_pro = opener.open(pro_request) --&gt; 携带登录后的Cookie，再次发送请求
    4. 输出标准格式的Cookie：cookieStr = &quot;&quot;
        for item in cookie:
            cookieStr = cookieStr + item.name + &quot;=&quot; + item.value + &quot;;&quot;
        print cookieStr[:-1]
3. 对于FileCookieJar，发送请求之后，执行cookie.save()，才能保存Cookie到本地文件；
    1. cookie = cookielib.MozillaCookieJar()
    2. cookie.load(&apos;cookie.txt&apos;)：从文件中读取Cookie到内存中。
4. 模拟登录的注意点：
    1. 登录一般都会先有一个HTTP GET，拉取一些信息及获得Cookie，然后再HTTP POST登录；
    2. HTTP POST登录的链接有可能是动态的，从GET返回的信息中获取；
    3. password有些是明文发送，有些是加密后发送；有些网站甚至采用动态加密的，同时包括了
    很多其他数据的加密信息，只能通过查看JS源码获得加密算法，再去破解加密，非常困难；
    4. 大多数网站的登录整体流程是类似的，可能有些细节不一样，不能保证每个网站都能登录成功。
</code></pre><h2 id="URLError与HTTPError"><a href="#URLError与HTTPError" class="headerlink" title="URLError与HTTPError"></a>URLError与HTTPError</h2><pre><code>urlopen()/opener.open()发出请求时，如果urlopen()/opener.open()不能处理这个response，
就产生错误，常见的包括URLError、HTTPError
1. URLError产生的主要原因：没有网络连接、服务器连接失败、找不到指定的服务器；
    1. 在发送请求时，对urlopen()/opener.open()使用try-except捕获相应的异常；
    2. urlopen error [Errno 8]：未找到指定的服务器。
2. HTTPError是URLError的子类，服务器响应的response包含响应码，但urllib2会自动处理重定向
   的页面(3开头的响应码)，100-299表示成功，所以只能看到400-599的错误码；
3. try-except优先处理HTTPError，然后处理URLError：
    try:
        urllib2.urlopen(requset)
    except urllib2.HTTPError, err:
        print err.code
    except urllib2.URLError, err:
        print err
</code></pre><h2 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h2><pre><code>requests模块的底层是urllib3，继承了urllib2的所有特性，号称：HTTP for Humans
1. requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传；
2. 支持自动确定响应内容的编码，支持国际化的URL和POST数据自动编码；
3. requests能完全满足当前网络的需求，支持python2.6-3.x，而且能运行在PyPy环境；
4. 安装：pip install requests，easy_install requests
</code></pre><h3 id="GET和POST"><a href="#GET和POST" class="headerlink" title="GET和POST"></a>GET和POST</h3><pre><code>1. GET请求：
    1. response = requests.get(&quot;http://www.baidu.com/&quot;)
    2. response = requests.request(&quot;get&quot;, &quot;http://www.baidu.com/&quot;)
2. POST请求：response = requests.post(url, data={&apos;k1&apos;:&apos;v1&apos;}, heanders)
3. 添加请求头与请求参数
    1. requests.get(url, params={&apos;k1&apos;:&apos;v1&apos;}, headers={&apos;k1&apos;:&apos;v1&apos;})
    2. params：GET请求需要传递的参数；    headers：请求头；
    3. params和headers都会被自动国际化编码，不需要手动处理。
4. response.url：查看完整的URL；    response.status_code：查看响应码；
5. response.text：获取响应内容，一般为Unicode编码的数据；
    1. response.encoding：查看响应内容的字符编码；
    2. response.encoding=&apos;utf-8&apos;：修改响应数据的编码，response.text也会使用此编码；
    3. 在写入文件时，如果报编码错误：UnicodeEncodeError:&apos;ascii&apos; codec can&apos;t ...
    则使用&apos;utf-8&apos;编码，再写入：
    with open(&apos;res.txt&apos;, &apos;w&apos;) as f:
        f.write(response.text.encode(&apos;utf-8&apos;))
6. response.content：获取原始二进制字节流形式的响应内容，可用来保存图片等二进制文件；
7. response.json()：如果响应的是JSON数据，可以直接获取。
</code></pre><h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><pre><code>1. 配置单个请求的代理：
    res = requests.get(url, proxies = {&apos;http&apos;:&apos;代理URL&apos;, &apos;https&apos;:&apos;代理URL&apos;})
2. 统一配置代理：在本地环境变量中配置HTTP_PROXY和HTTPS_PROXY
    export HTTP_PROXY=&quot;http://12.34.56.79:9527&quot;
    export HTTPS_PROXY=&quot;https://12.34.56.79:9527&quot;
3. 私密代理验证账号密码
    response = requests.get(url, proxies={ &quot;代理类型&quot;: &quot;用户名:密码@Ip:Port&quot; })
4. web客户端验证账号密码：response = requests.get(url, auth=(&quot;用户名&quot;, &quot;密码&quot;))
</code></pre><h3 id="Cookie与Sission"><a href="#Cookie与Sission" class="headerlink" title="Cookie与Sission"></a>Cookie与Sission</h3><pre><code>1. Cookie
    1. cookiejar = response.cookies：如果一个请求中包含Cookie，则获取到CookieJar对象；
    2. cookiedict = requests.utils.dict_from_cookiejar(cookiejar)：转为字典形式；
2. Sission
    1. 在requests中，Session对象是很常用的，它代表一次用户会话，Client连接Server开始，
    到Client断开连接；
    2. 会话能让Client在跨请求时保持某些参数，比如：在某一个Session实例发出的所有请求之间
    保持Cookie。
3. Session模拟Client登录
    ssion = requests.session() --&gt; 创建Session对象，保存Cookie；
    ssion.post(url, data={ 账号密码等参数 }, headers)
    response = ssion.get(url) --&gt; 登录成功后，ssion中已经保存了cookie，可以直接请求；
</code></pre><h3 id="HTTPS请求SSL证书"><a href="#HTTPS请求SSL证书" class="headerlink" title="HTTPS请求SSL证书"></a>HTTPS请求SSL证书</h3><pre><code>1. verify=True：表示发起HTTPS请求时，检查主机的SSL证书，默认为True；
    response = requests.get(&quot;https://www.baidu.com/&quot;, verify=True)
2. 如果SSL证书验证不通过/不信任Server的安全证书，如https://www.12306.cn，则报SSLError
    requests.get(&quot;https://www.12306.cn/mormhweb/&quot;, verify = False) --&gt; 跳过验证
</code></pre><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><pre><code>爬取页面的内容一般分为两部分：非结构化数据和结构化数据；
1. 非结构化数据：先有数据，再有结构；
    1. 文本、电话号码、邮箱地址的处理方式：正则表达式；
    2. HTML文件：正则表达式、XPath、CSS选择器。
2. 结构化数据：先有结构，再有数据；
    1. JSON的处理方式：JSONPath、转化成Python类型进行操作(Json类)；
    2. XML：转化成Python类型(xmltodict)、XPath、CSS选择器、正则表达式。
3. 抓取速度：正则 &gt; XPath &gt; CSS选择器，但CSS选择器的使用时最简单的。
</code></pre><h3 id="XPath"><a href="#XPath" class="headerlink" title="XPath"></a>XPath</h3><pre><code>1. XPath：是一门在XML文档中查找信息的语言，可用来在XML文档中对元素和属性进行遍历；
    1. /div：从根节点开始匹配&lt;div&gt;；    //div：从任意节点开始匹配&lt;div&gt;；
    2. //div[@class=&quot;threadlist&quot;]：从任意目录开始、根据指定的属性进行匹配div节点；
    3. //div/a：从任意节点开始匹配div节点，然后匹配&lt;div&gt;的直接子节点&lt;a&gt;；
    4. //div//a：从任意节点开始匹配div节点，然后匹配&lt;div&gt;下任意层级的子节点&lt;a&gt;；
    5. //div//a[@class=&quot;thie&quot;]/@href：获取匹配&lt;a&gt;的href属性值；
    6. //div//@href：直接匹配&lt;div&gt;下的所有节点的href属性值；
    7. //div[@class=&quot;li_txt&quot;]/h3/text()：获取&lt;h3&gt;的文本内容
    8. .：选取当前节点；    ..：选取当前节点的父节点；    *：选取所有节点；    |：多选。
2. lxml：将HTML文档解析为HTML DOM模型，即把HTML转为XML；该模块需要手动安装；
    1. from lxml import etree
    2. html = urllib2.urlopen(request).read()
    3. xml = etree.HTML(html)
    4. content = xml.xpath(&apos;XPath的查找规则&apos;) --&gt; 返回一个列表，类似于正则findall()
3. content = xml.xpath(&apos;//div/a&apos;)
    1. content[0].tag：获取标签名；    content[0].text：获取标签内的字符串文本；
    2. content[0].get(&apos;href&apos;)：获取指定的属性值；
    3. content[0].xpath(&apos;./span&apos;)：对&apos;//div/a&apos;匹配的内容进一步匹配，//div/a/span
4. 不同的WebServer会针对不同的Client发送不同的数据，一般IE是标准格式，使用XPath解析时，
   通常使用IE的User-Agent作为请求头；
5. 如果当前层级查找不到，则继续向上取父节点，如爬取百度贴吧的帖子链接：
    1. 无效的XPath规则：
    //div[@class=&quot;threadlist_lz clearfix&quot;]/div/a[@class=&quot;j_th_tit&quot;]/@href
    //div[@class=&quot;threadlist_lz clearfix&quot;]//a[@class=&quot;j_th_tit&quot;]/@href
    2. 有效的XPath规则：//div[@class=&quot;t_con cleafix&quot;]/div/div/div/a/@href
6. 强大的模糊查询：
    //div[contains(@id, &quot;qiush_tag_&quot;)]，匹配id属性值中包含&quot;qiush&quot;的&lt;div&gt;
</code></pre><h3 id="BeautifulSoup4"><a href="#BeautifulSoup4" class="headerlink" title="BeautifulSoup4"></a>BeautifulSoup4</h3><pre><code>CSS选择器：BeautifulSoup4，也是一个HTML/XML的解析器，主要用于解析和提取HTML/XML数据；
    1. lxml只会局部遍历，而BeautifulSoup基于HTML DOM，会载入整个文档，解析整个DOM树，
    因此时间和内存开销都会大很多，所以性能要低于lxml；
    2. BeautifulSoup解析HTML比较简单，支持CSS选择器、Python标准库中的HTML解析器，
    也支持lxml的XML解析器；
    3. BS3目前已经停止开发，推荐使用BS4：pip install beautifulsoup4
1. from bs4 import BeautifulSoup
    1. soup = BeautifulSoup(html) --&gt; 由爬取的HTML创建BeautifulSoup对象
    2. BeautifulSoup(open(&apos;index.html&apos;)) --&gt; 打开本地HTML文件创建BeautifulSoup对象
    3. soup = BeautifulSoup(html, &apos;lxml&apos;)：指定解析器为lxml，如果没有显示指定，默认
    使用当前系统的最佳可用HTML解析器，不同的系统/虚拟环境中，使用不同的解析器造成行为不同；
    4. print soup.prettify() --&gt; 格式化输出soup对象的内容；
2. BeautifulSoup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象
   可以归纳为4种：BeautifulSoup、Tag、NavigableString、Comment
</code></pre><h4 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h4><pre><code>1. soup.element：根据标签名获取标签对象，且只返回第一个，对象类型为bs4.element.Tag
    1. soup.title：&lt;title&gt; ... &lt;/title&gt;
    2. soup.head：&lt;head&gt;&lt;title&gt; ... &lt;/title&gt;&lt;/head&gt;
    3. soup.element.get_text()：获取标签内的字符串文本。
2. 属性name
    1. BeautifulSoup表示的是一个文档的内容，它是一个特殊的Tag，soup.name：[document]
    2. soup.element.name：返回标签名，如soup.title.name：title
3. 属性attrs：获取标签的所有属性组成的字典，如果该标签没有使用属性，则返回空字典；
    1. 标签的class属性可以指定多个值，以列表的形式返回；
    2. soup.p.attrs：获取&lt;p&gt;的所有属性，{&apos;class&apos;: [&apos;cls1&apos;, &apos;cls2&apos;], &apos;name&apos;: &apos;p1&apos;}
4. soup.p[&apos;attr&apos;]/soup.p.get(&apos;attr&apos;)：根据标签的属性名获取属性值；
    1. soup.p.get(&apos;name&apos;)：获取&lt;p&gt;的name属性值；
    2. 如果标签属性有多个属性值，如class属性，则返回一个属性值列表。
5. soup.p[&apos;class&apos;] = &apos;cls3&apos;：修改&lt;p&gt;的class属性值；
6. del soup.p[&apos;class&apos;]：删除&lt;p&gt;的class属性。
</code></pre><h4 id="NavigableString"><a href="#NavigableString" class="headerlink" title="NavigableString"></a>NavigableString</h4><pre><code>&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;
    1. soup.p.string：获取&lt;p&gt;&lt;/p&gt;的文本内容，The Dormouse&apos;s story
    2. type(soup.p.string)：&lt;class &apos;bs4.element.NavigableString&apos;&gt;
&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;
    1. Comment是一个特殊NavigableString，其输出的内容不包括注释符号；
    2. soup.a.string：获取&lt;a&gt;&lt;/a&gt;的文本内容，Elsie
    3. type(soup.a.string)：&lt;class &apos;bs4.element.Comment&apos;&gt;
&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;child1&lt;/b&gt;&lt;b&gt;child2&lt;/b&gt;&lt;/p&gt;
    soup.p.string：None，即当标签内只有一个直接子节点，或者没有子节点时，string才有效。
</code></pre><h4 id="遍历文档树"><a href="#遍历文档树" class="headerlink" title="遍历文档树"></a>遍历文档树</h4><pre><code>1. soup.element.contents：以列表的形式输出element的子节点的Tag对象，包括换行符\n等；
2. soup.element.children：返回一个列表迭代器listiterator，遍历获取所有子节点的Tag对象；
3. soup.element.descendants：content和children只遍历一次直接子节点，而descendants会
   递归遍历所有子节点，包括标签内的文本，返回一个生成器对象generator，遍历获取所有Tag对象；
</code></pre><h4 id="搜索文档树"><a href="#搜索文档树" class="headerlink" title="搜索文档树"></a>搜索文档树</h4><pre><code>soup.find(name, attrs, recursive, text, **kwargs)：返回第一个匹配标签的Tag对象；
soup.find_all(name, attrs, recursive, text, **kwargs)：返回一个列表；
1. 参数name：只针对标签，会自动过滤字符串文本；
    1. find_all(&apos;a&apos;)：返回所有&lt;a&gt;标签的Tag对象；
    2. find_all(re.compile(&apos;^b&apos;))：使用正则查找以&apos;b&apos;为开头的标签；
    3. find_all([&apos;a&apos;, &apos;b&apos;])：根据列表指定查找哪些标签。
2. 参数text：搜索标签内的字符串文本，返回匹配的字符串列表；
    1. text接受的参数与name相同：字符串、正则表达式、列表；
    2. find_all(text=re.compile(&apos;story&apos;))：匹配文档中包含&apos;story&apos;的字符串文本。
3. 参数attrs：根据标签的属性名和属性值进行查找，接收一个字典，多个键值表示条件与；
    1. find_all(attrs={&apos;id&apos;: &apos;uname&apos;})：查找id的属性值为&apos;uname&apos;的标签；
    2. find_all(attrs={&apos;class&apos;: &apos;con&apos;})：查找class的属性值中包含&apos;con&apos;的标签；
    3. find_all(attrs={&apos;name&apos;: &apos;uname&apos;, &apos;value&apos;: &apos;man&apos;})：查找name属性值为&apos;uname&apos;
    且value属性值为&apos;man&apos;的标签；
    4. find_all(&apos;h2&apos;, {&apos;class&apos;: &apos;ellip&apos;})：查找class的属性值包含&apos;ellip&apos;的&lt;h2&gt;
4. 参数kwargs：如果一个指定名字的参数不是搜索内置的参数名，则把该参数当作标签属性来搜索；
    1. find_all(id=&apos;login&apos;)：搜索id属性值为&apos;login&apos;的标签；
    2. find_all(href=re.compile(&quot;elsie&quot;))：搜索href属性值中匹配&apos;elsie&apos;的标签；
    3. find_all(id=True)：查找所有包含id属性的标签。
</code></pre><h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><pre><code>1. soup.select()：CSS选择器搜索文档树的方法，返回Tag列表，根据定义CSS名称的方式查找；
2. 标签选择器查找，即根据标签名查找：select(&apos;a&apos;)
3. 类选择器查找：select(&apos;.content&apos;)，匹配class属性值包含&apos;content&apos;的标签；
4. id选择器查找：select(&apos;#login&apos;)，匹配id为&apos;login&apos;的标签；
5. 利用层级选择器查找：select(&apos;.cont a&apos;)，使用了con选择器的标签下的&lt;a&gt;；
6. 只查找直接子节点：select(&apos;.cont &gt; a&apos;)，使用了con选择器的标签下的直接子节点&lt;a&gt;
</code></pre><h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><pre><code>1. python自带有json模块，import json，用于字符串和python数据类型间的转换；
2. json.loads(str)：把Json格式字符串解码转换为Python对象；
    object --&gt; dict，array --&gt; list，string --&gt; unicode str，null --&gt; None
    true/false --&gt; True/False
    1. 如果在loads()时出错，可能是因为Json字符串的编码不是UTF-8；
    2. 对于GBK编码的字符串：json.loads(jsonstr, encoding=&quot;GBK&quot;)
    3. 如果Json字符串指定了合适的编码，但是其中又包含其他编码的字符，则需要先转换为Unicode，
    然后再指定编码格式：
    unicodeStr = jsonStr.decode(&quot;GB2312&quot;); 
    data = json.loads(unicodeStr, encoding=&quot;GB2312&quot;);
    5. 任何平台的任何编码 都能和Unicode互相转换，对于UTF-8与GBK互相转换，先把UTF-8转换成
    Unicode(decode(&quot;UTF-8&quot;))，再从Unicode转换成GBK(encode(&quot;GBK&quot;))，反之同理；
    6. decode()：将其他编码的字符串转换成Unicode编码；
    7. encode()：将Unicode编码转换成其他编码的字符串。
3. json.dumps(obj)：把Python对象转为Json字符串，list/tuple --&gt; array
    1. dumps()转换时默认使用ASCII编码，dumps(obj, ensure_ascii=False)表示禁用ASCII，
    按UTF-8编码，但如果obj中是全英文的元素，仍使用ASCII编码；
    2. chardet：一个非常优秀的编码识别模块，可通过pip安装；
    3. chardet.detect(json.dumps(obj)) --&gt; {&apos;confidence&apos;:1.0,&apos;encoding&apos;:&apos;ascii&apos;}
4. json.dump(obj, open(&apos;f.json&apos;, &apos;w&apos;), ensure_ascii=False)：转为Json后写入文件；
5. json.load(open(&quot;f.json&quot;))：读取文件中Json，转为Python对象。
</code></pre><h4 id="JSONPath"><a href="#JSONPath" class="headerlink" title="JSONPath"></a>JSONPath</h4><pre><code>XPath用于匹配XML，JSONPath用于匹配Json，安装：pip install jsonpath
1. 语法对比
    1. $：根节点 --&gt; XPath的&apos;/&apos;；        @：当前节点 --&gt; XPath的&apos;.&apos;；
    2. .或[]：取子节点；    *：匹配所有节点；    [,]：多选操作 --&gt; XPath的&apos;|&apos;；
    3. ..：从任意节点开始查找 --&gt; XPath的&apos;//&apos;；    ?()：过滤操作 --&gt; XPath的&apos;[]&apos;；
    4. []：迭代器标示，可以做简单的迭代操作，如数组下标、根据内容选值等；
    5. XPath的[index]是从1开始，而JSONPath是从0开始，&apos;//book[3]&apos; &lt;--&gt; &apos;$..book[2]&apos;
2. import jsonpath
    1. city = json.loads(html)
    2. jsonpath.jsonpath(city, &apos;$..name&apos;)：从任意节点匹配name，返回一个name值的列表.
</code></pre><hr>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/03/爬虫基础-1/" rel="next" title="爬虫基础-1">
                <i class="fa fa-chevron-left"></i> 爬虫基础-1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/10/07/爬虫基础-3/" rel="prev" title="爬虫基础-3">
                爬虫基础-3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">大麦田怪圈</p>
              <p class="site-description motion-element" itemprop="description">敢做就能赢！</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">53</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#cookielib"><span class="nav-number">1.</span> <span class="nav-text">cookielib</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#URLError与HTTPError"><span class="nav-number">2.</span> <span class="nav-text">URLError与HTTPError</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#requests"><span class="nav-number">3.</span> <span class="nav-text">requests</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GET和POST"><span class="nav-number">3.1.</span> <span class="nav-text">GET和POST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理"><span class="nav-number">3.2.</span> <span class="nav-text">代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie与Sission"><span class="nav-number">3.3.</span> <span class="nav-text">Cookie与Sission</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTPS请求SSL证书"><span class="nav-number">3.4.</span> <span class="nav-text">HTTPS请求SSL证书</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#提取数据"><span class="nav-number">4.</span> <span class="nav-text">提取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#XPath"><span class="nav-number">4.1.</span> <span class="nav-text">XPath</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BeautifulSoup4"><span class="nav-number">4.2.</span> <span class="nav-text">BeautifulSoup4</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tag"><span class="nav-number">4.2.1.</span> <span class="nav-text">Tag</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NavigableString"><span class="nav-number">4.2.2.</span> <span class="nav-text">NavigableString</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#遍历文档树"><span class="nav-number">4.2.3.</span> <span class="nav-text">遍历文档树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#搜索文档树"><span class="nav-number">4.2.4.</span> <span class="nav-text">搜索文档树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSS选择器"><span class="nav-number">4.2.5.</span> <span class="nav-text">CSS选择器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#json"><span class="nav-number">4.3.</span> <span class="nav-text">json</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JSONPath"><span class="nav-number">4.3.1.</span> <span class="nav-text">JSONPath</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">大麦田怪圈</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
